1.原始数据格式
其中第一个数字为userId，其后的列表为该userId的关注用户的userId
[1799865655, [2869347900, 1798580325, 1642055823, 2799188791, 1779720113, 1685482501, 2202311361, 1655302873, 1743363694, 1654727873, 1398082037, 2133847855, 1355610915, 1814497153, 2392016215, 1756908533, 1745503947, 1812507707, 1785751701, 1621937643, 2198790500, 1962310741, 1594779263, 1261700994, 1592911662, 2060733221, 1230663070, 1808717020, 1695247672, 1618051664, 1670071920, 1189729754, 1182389073, 1858398714, 1705586121, 1821444155, 1195403385, 1724367710, 1845653780, 1779328684, 1843152063, 1926931587, 1912236297, 1644395354, 1689369091, 1749127163, 2157486862, 1741692611, 1644492510, 1709350393, 1698229264, 1995801167, 1182391231, 1713926427, 1736708753, 2134671703, 1787542433, 1800852695, 2074537511, 1707178184, 1640601392, 1811781304, 1734741902, 1642635773, 1400992111, 1741276322, 1682243591, 1812458977, 1407988324, 1670768954, 1717871843, 1883497310, 1197161814, 1666342717, 1614282004, 1954877773, 1282005885, 1957030507, 1750223583, 2125939111, 1889588180, 2016713117, 1768541657, 1642909335, 1732950433, 1831348402]]

[1799865655, []]

2.目标是将上述格式的数据转化成userID，count，flowees_list

3.数据预处理
1799865655	2869347900,1798580325,...,1831348402

1799865655	

http://blog.csdn.net/xiaolang85/article/details/51330634

sed -i -e 's/\,/\t/' -e 's/\[//g' -e 's/\]//g' followees.txt


create table  raw_followees(user_id string,followees array<string>) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t'
COLLECTION ITEMS TERMINATED BY ',';

load data inpath '/user/bd/weibo/followees_10.txt' overwrite into table raw_followees;  

create table not_0_followees as select user_id,size(followees) as num,followees from raw_followees where size(followees) != 1;

insert overwrite local directory '/mnt/sdb1/weibo/followees'
row format delimited
fields terminated by '\t'
COLLECTION ITEMS TERMINATED BY ','
select * from followees_table;

create table uid_num_followees as select uid,size(followees) as num,followees from followees_table;

初步统计：
关注关系最多的用户个数:3766
有关注关系的用户数量:7960069
关注人:5-1000：6918907
      10-1000:6862724
      100-1000:5544229

将用户发微博数量表和用户关注关系表join：
id num 关注关系
create table id_weiboNum_floweesNum_followees as select a.uid,b.weibonum weiboNum,a.num followeesNum,a.followees from uid_num_followees a left outer join id_weibonum b on a.uid = b.id;

create table user_profile_new as select cast(user_id as bigint) user_id,nick_name,gender,address,description,tags,blog,birthday,domain,primary_school,middle_school,university,company,blood_type,email,qq from user_profile where user_id is not NULL;

create table id_weibonum_floweesnum_tags_followees as select a.uid,a.weibonum,a.followeesnum,b.tags,a.followees from id_weibonum_floweesnum_followees a left outer join user_profile_new b on a.uid = b.user_id;

select count(*) from id_weibonum_floweesnum_tags_followees where weibonum > 200 and weibonum < 250 and followeesnum > 100 and followeesnum < 200;

select uid,followee from id_weibonum_floweesnum_tags_followees_test laternal view exploid(followees) as followee;

create table uid_followee_sample as select uid,followee,1 from id_weibonum_floweesnum_tags_followees lateral view explode(followees) mytable as followee where weibonum > 200 and weibonum < 250 and followeesnum > 50 and followeesnum < 200;

create table uid_sample as select uid from uid_followee_sample;

insert into table uid_sample select followee as uid from uid_followee_sample;

create table uid_distinct_sample as select distinct(uid) uid from uid_sample;

insert overwrite directory '/user/bd/uid_followee_sample' row format delimited fields terminated by '\t' select * from uid_followee_sample;

create table uid_tags as select a.uid,b.tags from uid_distinct_sample a left outer join id_weibonum_floweesnum_tags_followees b on a.uid = b.uid;

create table uid_tag_sample as select uid,tag,1 from uid_tags_train_set lateral view explode(split(tags," ")) mytable as tag where tags is not NULL;

insert overwrite directory '/user/bd/uid_tag_sample' row format delimited fields terminated by '\t' select * from uid_tag_sample where trim(tag)!="";

./reconstruct -train /home/bd/LINE/data/uid_followee_sample.txt -output uid_followee_sample_dense.txt -depth 2 -k-max 1000

./reconstruct -train /home/bd/LINE/data/uid_tag_sample.txt -output uid_tag_sample_dense.txt -depth 2 -k-max 1000



----------------------找隐含的tag----------------------------
1、把分词后的数据导入到hdfs
hadoop fs -put 1_uid_token.txt /user/bd/weibo_token
hadoop fs -put 2_uid_token.txt /user/bd/weibo_token
hadoop fs -put 3_uid_token.txt /user/bd/weibo_token
hadoop fs -put 4_uid_token.txt /user/bd/weibo_token
hadoop fs -put 5_uid_token.txt /user/bd/weibo_token
hadoop fs -put 6_uid_token.txt /user/bd/weibo_token
hadoop fs -put stopwords.txt /user/bd/


2、
create table  uid_tokens(user_id string,tokens array<string>) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t'
COLLECTION ITEMS TERMINATED BY ' ';

create table stopwords(stopword string) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t'
COLLECTION ITEMS TERMINATED BY ' ';

3.
load data inpath '/user/bd/weibo_token/*' overwrite into table uid_tokens; 
load data inpath '/user/bd/stopwords.txt' overwrite into table stopwords;  


4.先过滤需要研究的用户,需要uid_tokens和uid_tags_train_set

create table uid_tokens_new as select cast(user_id as bigint) user_id,followees as tokens from uid_tokens where user_id is not NULL;

create table uid_tokens_sample as select * from uid_tokens_new where user_id in (select uid from uid_tags_train_set);

5.将tokens行转列
create table uid_token as select user_id,token from uid_tokens_sample lateral view explode(tokens) mytable as token where token is not NULL;

6.按照uid、token分组，并统计组内个数
create table uid_token_num as select user_id,token,count(*) from uid_token group by user_id,token;

7.从uid_tag_sample中找出所有不同的tag
create table tags as select distinct(tag) as tag from uid_tag_sample;

8.找出是tag的token
create table uid_token_num_filtered_by_tags as select user_id,token,`_c2` num from uid_token_num where token in (select tag from tags);

9.对uid_token_num_filtered_by_tags和uid_tag_sample过滤停用词

create table uid_tag_sample_rm_stwords as select * from uid_tag_sample where tag not in (select stopword from stopwords);

create table uid_token_num_filtered_rm_stwords as select * from uid_token_num_filtered_by_tags where token not in (select stopword from stopwords);

10.隐含tag的数量
select count(*) from uid_token_num_filtered_rm_stwords where num > 20 and num < 100 and length(token) > 1;

11.生成隐含tag的目标结果
create table uid_token_sample_0_0_1 as select user_id,token, round(num * 0.01,2) as weight from uid_token_num_filtered_rm_stwords where num > 20 and num < 100 and length(token) > 1;

create table uid_token_sample_0_0_0_5 as select user_id,token, round(num * 0.005,2) as weight from uid_token_num_filtered_rm_stwords where num > 20 and num < 100 and length(token) > 1;

12.生成直接tag的目标结果
create table uid_tag_sample_direct as select uid,tag, `_c2` as weight from uid_tag_sample_rm_stwords where length(tag) > 1;

13.导出结果
insert overwrite directory '/user/bd/uid_token_sample_0_0_1' row format delimited fields terminated by '\t' select * from uid_token_sample_0_0_1;

insert overwrite directory '/user/bd/uid_token_sample_0_0_0_5' row format delimited fields terminated by '\t' select * from uid_token_sample_0_0_0_5;

insert overwrite directory '/user/bd/uid_tag_sample_direct' row format delimited fields terminated by '\t' select * from uid_tag_sample_direct;

-----------------------找at关系-------------------------------------------------
1.创建外部表
create external table at_relation(uidA bigint,uidB bigint,num int) 
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY '\t'
COLLECTION ITEMS TERMINATED BY ',';
2.数据备份到hdfs
hadoop fs -put /home/bd/LINE/data/at_relation_sort.txt /user/bd
3.向hive中导入数据
load data inpath '/user/bd/at_relation_sort.txt' overwrite into table at_relation;

4. 对at_relation的uida的边按照uid_distinct_sample中的uid进行过滤,剩余13741995
create table at_relation_1_ver as select * from at_relation where uida in (select uid from uid_distinct_sample);

5. 对at_relation的uidb的边按照uid_distinct_sample中的uid进行过滤,剩余10678239
create table at_relation_2_ver as select * from at_relation_1_ver where uidb in (select uid from uid_distinct_sample);


----------------------将带标签用户分为训练集和测试集--------------------------------
1.uid_tags为样本用户的标签集合，如果没有标签的话，标签为NULL,通过下面得到测试集合uid_tags_test_set

create table uid_tags_test_set as select * from uid_tags where tags is not NULL order by rand(12312323) limit 50000;

2.测试集和训练集的并集
create table uid_tags_all_set as select * from uid_tags where tags is not NULL;

3.训练集
create table uid_tags_train_set as select * from uid_tags_all_set where uid not in (select uid from uid_tags_test_set); 

create table uid_tag_sample as select uid,tag,1 from uid_tags_train_set lateral view explode(split(tags," ")) mytable as tag where tags is not NULL;

insert overwrite directory '/user/bd/uid_tag_sample' row format delimited fields terminated by '\t' select * from uid_tag_sample where trim(tag)!="";

----------------------计算ground truth-------------------------------------------
1.涉及user_profile_new、uid_tags_test_set、id_weibonum_floweesnum_followees
  把uid_tags_test_set和id_weibonum_floweesnum_followees取join
  create table uid_tags_followees_test as select a.uid,a.tags,b.followees from uid_tags_test_set a left outer join id_weibonum_floweesnum_followees b on a.uid = b.uid;

2.把uid_tags_followees_test的followees行转列
  create table uid_tags_uid2_test as select uid,tags,uid2 from uid_tags_followees_test lateral view explode(followees) mytable as uid2;

3.利用uid_tags_uid2_test和user_profile_new来join，并取得uid2对应的tags
  create table uid_tags_uid2_tags2_test as select a.uid,a.tags,a.uid2,b.tags tags2 from uid_tags_uid2_test a left outer join user_profile_new b on a.uid2 = b.user_id;

4.去掉uid2 = 0的异常数据
  create table uid_tags_uid2_tags2_test_new as select * from uid_tags_uid2_tags2_test where uid2 != 0;

5.将tags2行转列
  create table uid_tags_uid2_tag as select uid,tags,uid2,tag from uid_tags_uid2_tags2_test_new lateral view explode(split(tags2," ")) mytable  as tag where tag is not NULL;

6.对uid_tags_uid2_tag根据uid和tag进行分组并统计每组的个数
  create table uid_tags_uid2List_tag_num as select uid,tags,collect_set(uid2) as uid2List,tag,count(tag) as num from uid_tags_uid2_tag group by uid,tag,tags;

7.对uid_tags_uid2List_tag_num按照uid分组，并按照order降序排列，后取得前5个tag(存在并列组号)
  create table uid_tags_tag_num_rank_top5 as select uid,tags,tag,num,rank from (select uid, tags, tag,num, row_number() over ( partition by uid order by num desc) as rank from uid_tags_uid2List_tag_num ) t where rank < 6;

8.将top5的结果列转行生成ground_truth_5（存在并列组号）
  create table uid_tags_predicTags_top5  as select uid,tags,collect_set(tag) as predictList from uid_tags_tag_num_rank_top5 group by uid,tags; 

9.对uid_tags_uid2List_tag_num按照uid分组，并按照order降序排列，后取得前5个tag(不存在并列组号)
  create table uid_tags_tag_num_row_num_top5 as select uid,tags,tag,num,rank from (select uid, tags, tag,num, row_number() over ( partition by uid order by num desc) as rank from uid_tags_uid2List_tag_num ) t where rank < 6;

  create table uid_tags_tag_num_row_num_top10 as select uid,tags,tag,num,rank from (select uid, tags, tag,num, row_number() over ( partition by uid order by num desc) as rank from uid_tags_uid2List_tag_num ) t where rank < 11;

  create table uid_tags_tag_num_row_num_top20 as select uid,tags,tag,num,rank from (select uid, tags, tag,num, row_number() over ( partition by uid order by num desc) as rank from uid_tags_uid2List_tag_num ) t where rank < 21;

  create table uid_tags_tag_num_row_num_top40 as select uid,tags,tag,num,rank from (select uid, tags, tag,num, row_number() over ( partition by uid order by num desc) as rank from uid_tags_uid2List_tag_num ) t where rank < 41;

  create table uid_tags_tag_num_row_num_top50 as select uid,tags,tag,num,rank from (select uid, tags, tag,num, row_number() over ( partition by uid order by num desc) as rank from uid_tags_uid2List_tag_num ) t where rank < 51;

  create table uid_tags_tag_num_row_num_top100 as select uid,tags,tag,num,rank from (select uid, tags, tag,num, row_number() over ( partition by uid order by num desc) as rank from uid_tags_uid2List_tag_num ) t where rank < 101;


10.将top5的结果列转行生成ground_truth_5（不存在并列组号）
  create table uid_tags_predicTags_top5_row  as select uid,tags,collect_set(tag) as predictList from uid_tags_tag_num_row_num_top5 group by uid,tags; 

  create table uid_tags_predicTags_top10_row  as select uid,tags,collect_set(tag) as predictList from uid_tags_tag_num_row_num_top10 group by uid,tags; 

  create table uid_tags_predicTags_top20_row  as select uid,tags,collect_set(tag) as predictList from uid_tags_tag_num_row_num_top20 group by uid,tags; 

  create table uid_tags_predicTags_top40_row  as select uid,tags,collect_set(tag) as predictList from uid_tags_tag_num_row_num_top40 group by uid,tags; 

  create table uid_tags_predicTags_top50_row  as select uid,tags,collect_set(tag) as predictList from uid_tags_tag_num_row_num_top50 group by uid,tags; 

   create table uid_tags_predicTags_top100_row  as select uid,tags,collect_set(tag) as predictList from uid_tags_tag_num_row_num_top100 group by uid,tags; 



11.将uid_tags_predicTags_top5_row结果输出(12.74%)
insert overwrite directory '/user/bd/uid_tags_predicTags_top5' row format delimited fields terminated by '\t' COLLECTION ITEMS TERMINATED BY ' ' select * from uid_tags_predicTags_top5_row;

  将uid_tags_predicTags_top10_row结果输出(15.67%)
insert overwrite directory '/user/bd/uid_tags_predicTags_top10' row format delimited fields terminated by '\t' COLLECTION ITEMS TERMINATED BY ' ' select * from uid_tags_predicTags_top10_row;

 将uid_tags_predicTags_top20_row结果输出(17.71%)
insert overwrite directory '/user/bd/uid_tags_predicTags_top20' row format delimited fields terminated by '\t' COLLECTION ITEMS TERMINATED BY ' ' select * from uid_tags_predicTags_top20_row;

将uid_tags_predicTags_top40_row结果输出(18.89%)
insert overwrite directory '/user/bd/uid_tags_predicTags_top40' row format delimited fields terminated by '\t' COLLECTION ITEMS TERMINATED BY ' ' select * from uid_tags_predicTags_top40_row;

将uid_tags_predicTags_top50_row结果输出(map:19.14% mrr:39.82% p@50:6.56%)
insert overwrite directory '/user/bd/uid_tags_predicTags_top50' row format delimited fields terminated by '\t' COLLECTION ITEMS TERMINATED BY ' ' select * from uid_tags_predicTags_top50_row;

将uid_tags_predicTags_top100_row结果输出(map:19.64% mrr:39.86% p@100:4.55%)
insert overwrite directory '/user/bd/uid_tags_predicTags_top100' row format delimited fields terminated by '\t' COLLECTION ITEMS TERMINATED BY ' ' select * from uid_tags_predicTags_top100_row;

---------------计算pte的准确率(除去5W测试数据外都是训练数据)----------------------
1.判断测试集uid_tags_test_set中是否所有用户都在uid_distinct_sample表中
select count(*) from uid_tags_test_set where uid not in (select uid from uid_distinct_sample);
由于结果为0，所以表明测试集中的所有用户均可用pte方法进行预测
2.uid_tags_predicTags_top5_pte.txt的准确率为0.14%

通过实验结果可以看出，由于异构用户标签网络的训练方法需要在所有标签集合中寻找与用户相似的标签，所以准确率不高。但是尝试结合基于用户关系预测用户标签的算法和有向异构用户标签网络嵌入式表示调节式算法的结合算法，在准确率上较baseline提升22.9%。





------------------------对tag进行清洗，提高准确率---------------------------------
需要的数据有uid_tags_train_set,uid_tag_sample两个表

1.计算每个tag的词频
create table tag_num as select tag,count(*) as num from uid_tag_sample group by tag;

2.选出词频大于20的tag作为备选tag,共15561.
create table tag_num_20 as select tag,num from tag_num where num > 20;
  选出词频大于15的tag作为备选tag,共20305.
create table tag_num_15 as select tag,num from tag_num where num > 15;
  选出词频大于10的tag作为备选tag,共29103.
create table tag_num_10 as select tag,num from tag_num where num > 10;
  选出词频大于50的tag作为备选tag,共6541.
create table tag_num_50 as select tag,num from tag_num where num > 50;
  选出词频大于100的tag作为备选tag,共3272.
create table tag_num_100 as select tag,num from tag_num where num > 100;

3.按照tag_num_20对uid_tag_sample进行清洗,并导出。

create table uid_tag_sample_20 as select uid,tag,1 weight from uid_tag_sample where tag in (select tag from tag_num_20);

insert overwrite directory '/user/bd/uid_tag_sample_20' row format delimited fields terminated by '\t' select * from uid_tag_sample_20 where trim(tag)!="";

4.按照tag_num_10对uid_tag_sample进行清洗,并导出。

create table uid_tag_sample_10 as select uid,tag,1 weight from uid_tag_sample where tag in (select tag from tag_num_10);

insert overwrite directory '/user/bd/uid_tag_sample_10' row format delimited fields terminated by '\t' select * from uid_tag_sample_10 where trim(tag)!="";

5.按照tag_num_50对uid_tag_sample进行清洗,并导出。

create table uid_tag_sample_50 as select uid,tag,1 weight from uid_tag_sample where tag in (select tag from tag_num_50);

insert overwrite directory '/user/bd/uid_tag_sample_50' row format delimited fields terminated by '\t' select * from uid_tag_sample_50 where trim(tag)!="";

6.按照tag_num_100对uid_tag_sample进行清洗,并导出。

create table uid_tag_sample_100 as select uid,tag,1 weight from uid_tag_sample where tag in (select tag from tag_num_100);

insert overwrite directory '/user/bd/uid_tag_sample_100' row format delimited fields terminated by '\t' select * from uid_tag_sample_100 where trim(tag)!="";



4.迭代次数100000M:top5:7.4258295E-4
                 top10:9.0391317E-4
                 top20:0.0010407555
                 top40:0.0011733873

5.只考虑一阶：
          top5: 0.008452911
          top10:0.009621175
          top20:0.010469124
          top40:0.011071548


6.只考虑二阶：top5:3.5195332E-4
            top10:4.517364E-4
            top20:5.5384723E-4
            top40:6.5028935E-4
7.一阶和二阶均考虑为 top5:0.24%
                 top10: 0.29%
                 top20:0.33%
                 top40:0.36%

---------------------------------出现词频高的词设置高权重进行优化(权重为词频)------------------------------------

1.涉及到的表有tag_num，uid_tag_sample 按照tag做join
create table uid_tag_freq as select a.uid,a.tag,b.num from uid_tag_sample a left outer join tag_num b on a.tag = b.tag; 
2.将uid_tag_freq 进行导出
insert overwrite directory '/user/bd/uid_tag_freq' row format delimited fields terminated by '\t' select * from uid_tag_freq where trim(tag)!="";

3. 一阶和二阶均考虑为(训练10000M) top5:3.4445466E-4
                               top10: 3.862824E-4
                               top20:4.1775263E-4
                               top40:4.465575E-4
   一阶和二阶均考虑为(训练5000M,一阶100维度，二阶50维)  top5:3.75687E-4  0.000375687
                                                  top10: 4.1611947E-4
                                                  top20:4.5139802E-4
                                                  top40:4.8349125E-4      

----------------------------------------优化负采样算法---------------------------------                   

1.一阶和二阶均考虑为(训练5000M,一阶100维度，二阶50维)  
top5: 0.0041533536
top10:0.004636838
top20:0.004981038
top40:0.0052422304

2.一阶和二阶均考虑为(训练5000M,一阶100维度，二阶50维),将异构图的degree分离计算，tune方式
top5: 0.0027796808
top10:0.0030401954
top20:0.003239623
top40:0.003376254

3.一阶和二阶均考虑为(训练5000M,一阶100维度，二阶50维),将异构图的degree分离计算，joint方式
top5: 3.9870312E-4
top10:4.748699E-4
top20:5.3828466E-4
top40:6.1055075E-4

4.负采样算法优化后+tag权重都是1+过滤词频低于20的tag+100+50+tune
top5: 0.0101237735
top10:0.012944515
top20:0.01589209
top40:0.018974675

5.负采样算法优化后+tag权重都是1+过滤词频低于20的tag+100+50+tune-undirect
top5:0.0055589397
top10:0.0068503385
top20:0.00784117
top40:0.008632835

6.负采样算法优化后+tag权重都是1+过滤词频低于20的tag+100+50+joint
top5:7.9741137E-4

6.负采样算法优化后+tag权重都是1+过滤词频低于20的tag+100+50+joint-undirect
top5:0.010385786
top10:0.012104012
top20:0.0133370785
top40:0.014309617
------------------------通过上面对比-----------------------------
认为pte-tune方法结果好些，然后调整参数
1.负采样算法优化后+tag权重都是1+过滤词频低于20的tag+100+100+tune


-----------------------Combine算法设计---------------------------
1.找到基于协同过滤所有备选词

create table uid_tags_cf_tags as select uid,tags,concat_ws(',',collect_set(tag)) as cf_tags from uid_tags_uid2_tag group by uid,tags;

2.找到备选集合中词语的词向量，如何存储？

gcc -lm -pthread -Ofast -march=native -Wall -funroll-loops -ffast-math -Wno-unused-result conbine_find_sim.c -o CombineFindSim

./CombineFindSim ../data/user_vec_1st_pte_tune.txt ../data/tag_vec_1st_pte_tune.txt uid_tags_predicTags_top100.txt combine.txt 100














































----------------------------------------------------------
目前有两个问题：
1.微博转发文本，没有转发来自谁。所以不能用来扩充用户关系网络，我打算用微博@关系来扩充关系网络。

2.结果验证的方法:
  总的方向：将有标签的用户分为训练集和测试集，通过PTE训练得到所有用户标签，然后计算准确率、召回率
           来验证是否比单独训练用户网络和标签网络的预测结果好，然后在以下4个数据集上验证这一结论
  1）利用未拓展的用户关系、未拓展的标签网络计算准确率、召回率
  2）利用拓展的用户关系、未拓展的标签网络计算准确率、召回率
  3）利用未拓展的用户关系、拓展的标签网络计算准确率、召回率
  4）利用拓展的用户关系、拓展的标签网络计算准确率、召回率

















































